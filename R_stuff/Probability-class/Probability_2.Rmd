---
title: "Math Club - Probability, Part 2"
author: "Mike Williamson"
date: "November 14^th^, 2017"
runtime: shiny
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
---


This document is a continuation of the Math Club - Probability, Part 1.

# Looking at Different Distributions

## Uniform Distribution

We've been talking about "the roll of a die" or "flipping a coin".  We've also looked at calculating the ***combinations*** of probabilites, whether as a *union* or as succeeding (*"given that"*) events.  In all these scenarios, the ***likelihood of each outcome is the same***.  This is referred to as a ***uniform distribution***.

For instance, it's the same chance ($\frac{1}{6}$) to roll a 1, or a 2, or any of the numbers on the 6-sided die.



## Playing with Uniform Distribution Sample Size

When we played the "gambling game" (mentioned in the final section, "Games", of the last lesson), I specifically had you all roll the dice many times over.  This is because I wanted enough rolls so that things would turn out "as expected".

Let's simulate that with the interactive random number generator below.  Below is a plot which tallies the number of times each outcome occurred.

E.g., if you choose $10$ tries and a 6-sided die, and the observed count for the number $2$ is $4$, then that means the value $2$ showed up $4$ times.  In other words, $40\ \%$ of the times you rolled the die, the die value was $2$.

But on a 6-sided die, the probability of rolling a $2$ is $\frac{1}{6}$; we just finished talking about that.  If the probability is only $\frac{1}{6}$ (or $16.67\ \%$), then **how come the $2$ actually showed up $40\ \%$ of the time??**

Play around with the simulation below to see for yourself.

```{r unif, echo=FALSE}
shinyAppFile("./Probability_2_uniform.R",
  options = list(
    width = "100%", height = 700
  )
)
```

As you can see by playing around with the number of tries above, sometimes even 100 tries, or observations, aren't enough to give the "real" shape of the **uniform distribution**, where we know each outcome is equally likely.

We **know** the probability is the same for each number to appear, and that the probability is $\frac{1}{6}$, or `r signif(1/6, 4)`, for the case of a 6-sided die.  But until the number of samples gets quite large, say around 1000 or so, we don't really see the lines flatten out and settle at $\frac{1}{6}$ very well.

-----

## Normal Distribution

You might have to think about it a bit at first, but then you'll realize there are a **ton** of cases where things are **random, but not uniform**:

* The height of 10 year olds
    * Very few are 3\` tall or 6\` tall, but many of you are around 4.5\` tall or so.

* The temperature mid-day in May by us is typically around 70$^{\circ}$.  It might be as cold as 55$^{\circ}$ every now and again, but it won't dip down to 45$^{\circ}$.

* The time it takes to finish your homework.  One week it might take only 45 minutes for the whole week, the next week it might take a full 2 hours, but it won't ever take 100 hours.

* Etc.

These sorts of distributions, **where things occur more often "near the middle, typical value" are called ["*Normal*"](http://en.wikipedia.org/wiki/Normal_distribution) or "*Gaussian*" distributions**.

Below is a mock-up of one such *normal distribution*:

> the average high temperature for a particular day here in San Mateo.

It should maybe make sense that this would follow a normal distribution.  Normal distributions are those where certain values are more likely, and other values are less likely as they drift further from that central, more likely value.

If the "typical" temperature is, say, 70^o^, then 69^o^ or 68^o^ is not unexpected, but 50^o^ is.

```{r norm, echo=FALSE}
shinyAppFile("./Probability_2_normal.R",
  options = list(
    width = "100%", height = 700
  )
)
```


*(For the curious, I found that the historical average high on May 15th is 70$^{\circ}$, and that the lowest recorded high was 50$^{\circ}$.  Assuming $\sim$ 3000 measurements - 150 years' data, where about 20 days in May are approximately the same - a standard deviation of around 6 would yield a "lowest high" of 50$^{\circ}$.  So I said the standard deviation is 6$^{\circ}$.)*

-----

## Binomial Distribution

As we've seen, the **normal** distribution has a symmetric, bell-curve shape.

The **uniform** distribution is, well, uniform.  That is, it is flat, with the same probability for each value.

But there are other random distributions that can occur.  Namely, the ***[binomial distribution](http://en.wikipedia.org/wiki/Binomial_distribution)***, which we'll look at now.  It looks something like:

```{r ex_binom, echo=FALSE}
renderPlot({
  ggplot(data.frame(Whatever=rbinom(1e5, 100, 0.025)), aes(x = Whatever)) +
    geom_histogram(aes(y= ..density.., fill = ..count..),
                   color = "darkgreen", binwidth = 1) +
    guides(fill = FALSE) + 
    labs(title = "Binomial Distribution", y = "Proportion", x = "") +
    scale_fill_gradient(low="white", high="green") + 
    theme(text = element_text(size = 23))
})
```

In fact, we cheated a little bit in the earlier normal distribution examples.  The time it takes for you to finish your homework is closer to a binomial distribution (or actually, a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution)) than a normal distribution. We'll talk more about why that is, but here is a clue: normal distributions have long "tails" on both sides, but it can never take you less than $0\ hours$ to finish your homework, so it cannot have the long tails on both sides.


### Back to Gambling!

As mentioned above, the binomial distribution is useful in many cases. But all of them are based around tallies of successes and failures.

So, the binomial distribution allows us to go back to our gambling obsession.

Let's go back to our flipping of heads or tails on a coin.  I have a $50\%$ chance of getting a heads.  And I have a $25\%$ chance of getting two heads in a row.  Sure, we figured that out earlier.  Easy enough.

> But what's my chance of getting 2 heads out of 3 coin flips?

> Or what about getting 6 ***or more*** heads out of 10 coin flips?

Well, before we jump into the technical verbiage, let's think about some of the intuitive things we know.  If I set up a generic expression along the lines of:

> What's the chance of getting "X" successes out of "Y" attempts?

then what can I say about $X$, $Y$, and $P$?

- If $X > Y$, then $P = 0$.
    - Well, that's kinda obvious.  For instance, I can't get, say, 12 heads from only 10 coin flips, right?
- If $X = Y$ or $X = 0$, then there is **only** one way that can happen.
    - E.g., there is only one way I can get 8 heads from 8 coin flips... each flip must be a head.
- If $0 < X < Y$, then there will be **more than** one way that can happen.
    - E.g., if I need 4 heads from 7 coin flips, I could first get a tails, then a heads, then maybe 3 more tails in a row, and 2 more heads.  I could also get all 4 tails in a row, then all heads.  Etc.

-----

### A Simple Example

OK, so let's look again at the proposal:

> What's my chance of getting 2 heads out of 3 coin flips?

First, there are ***3 successive events*** happening, and each event has a ***0.5 probability***.

How did I get that?

- There are 3 coin flips
- Whether the coin reveals heads or tails, it is still a $50\%$ chance.
    - Later we'll talk about cases where one of the outcomes is more likely than the other.

So, using our knowledge so far, we have, for any particular case (say, 2 heads and then a tails):

$$P_{HHT} = P\left(heads\right) \times P\left(heads\right) \times P\left(tails\right)$$

$$P_{HHT} = \frac{1}{2} \times \frac{1}{2} \times \frac{1}{2} = \frac{1}{8}$$

But, we know that there are ***other ways*** to get 2 heads out of 3 coin flips.  For instance, we could get a tails first, and then 2 heads:

$$P_{THH} = P\left(tails\right) \times P\left(heads\right) \times P\left(heads\right)$$

We know from our **"Adding" Probabilities** section that we will need to *union* these possible scenarios.  Moreover, we know that if "heads-heads-tails" happens, then "tails-heads-heads" cannot happen, and vice versa.  In other words, these possibilities are *mutually exclusive*, so that when we *union* these, we can simply add them.

But there are yet more possibilities to get 2 heads.  We could also get "heads-tails-heads".  Those are all the possible ways to get 2 heads from 3 coin flips.

So:

$$P\left(2\ heads\ \mid\ 3\ flips\right) = P_{HHT} + P_{THH} + P_{HTH}$$

$$P\left(2\ heads\ \mid\ 3\ flips\right) = \frac{1}{8} + \frac{1}{8} + \frac{1}{8} = \frac{3}{8}$$


We got our answer:

> What's my chance of getting 2 heads out of 3 coin flips?  It's $\frac{3}{8}$!

-----

### Calculating Probability for Binomial Distribution

But what about

> the chance of getting, say, 4 heads out of 10 coin flips??

We *could* list out all the possibilities, but there has to be an easier way.  In fact, there is!  The **[binomial coefficient](http://en.wikipedia.org/wiki/Binomial_coefficient)**, or **[Pascal's triangle](http://en.wikipedia.org/wiki/Pascal%27s_triangle)**, provides an easy way to capture the number of possibilities:

$$\binom{n}{k} = \frac{n!}{k! \times \left(n - k\right)}$$

Plugging this into our 2 heads out of 3 coin flips, we get:

$$\binom{3}{2} = \frac{3!}{2! \times \left(3 - 2\right)!} = \frac{3 \times 2!}{2! \times 1!} = 3$$

Woot!  We get the right answer!  (Remember, we had 3 possible ways of getting 2 heads from three flips:  HHT, HTH, THH.)

Remember, we pronounce the binomial coefficient using the expression "choose".  E.g., "3 choose 2" is $\binom{3}{2}$.

OK, now with 4 heads out of 10 coin flips:

$$\binom{10}{4} = \frac{10!}{4! \times \left(10 - 4\right)!} = $$

$$\frac{10 \times 9 \times 8 \times 7 \times 6!}{4! \times 6!} = $$

$$\frac{10 \times 9 \times 8 \times 7}{4!} = $$

$$\frac{5040}{24} = 210$$

So, there are 210 different ways to get 4 heads from 10 coin flips.  Whew!  Good thing we didn't try to list them all!

Now, let's calculate the likelihood of any of those ways happening.  Since they are all the same, let's just choose 4 heads, then 6 tails:

$$P_{HHHHTTTTTT} = \frac{1}{2}^{10} = 0.000977$$

With 210 possibilities, since we know they are all ***mutually exclusive***, we can add them all up.  And since they are all equally likely (i.e., the chance for $P_{HHHHTTTTTT}$ is the same as for $P_{TTHTTHHTTH}$), they will all have the same probability.

So...

> What is the chance of getting 4 heads out of 10 coin flips??

> $P\left(4\ heads \mid 10\ coin\ flips\right) = 210 \times 0.000977 = 0.205$

Or just over 20% chance of getting 4 heads out of 10 coin flips.

-----

### Pascal's Triangle

Quick diversion, some fun with the **binomial coefficient**:  the binomial coefficient follows Pascal's Triangle.  For example, below is Pascal's Triangle going all the way from 0 - 10 different coin flips:

```{r echo=FALSE}
pascalTriangle <- function(h) {
  for(i in 0:(h-1)) {
    s <- as.character(i)
    for(k in 0:(h-i)) s <- paste(s, "  ", sep="")
    for(j in 0:i) {
      s <- paste(s, sprintf("%3d ", choose(i, j)), sep="")
    }
    print(s)
  }
}
pascalTriangle(11)
```

To generate it, you start with just the number 1 on the top row (for 0 trials), then the pair of numbers 1 & 1 on the second row (for 1 trial, which could either succeed or fail).

Every row after that is made by combining the values above it, and adding a 1 on each end.  For instance, the third row (2 trials) will have $1 + 1 = 2$ as it's middle value, and is bookended by 1's.  The fourth row (3 trials), will have $1 + 2 = 3$ in both of the cases where that happens, bookended by 1's.  Etc.

Each column on each row is the number of successes, starting with none and ending with all of them.  So, for the 11th row (10 trials), we have:

$$1\ 10\ 45\ 120\ 210\ 252\ 210\ 120\ 45\ 10\ 1$$

Where the $1$'s on the ends represent, respectively, no heads and all heads.  The $10$ represents 1 head (or 9 heads), $45$ is 2 heads, $120$ is 3 heads, and $210$ is 4 heads.

-----

### The Shape of the Binomial Distribution

The binomial distribution has a few more variables to futz with, in order to describe it, than we've seen in our other distributions.

Let's think through this via the previous example, the chance of getting 4 heads out of 10 coin flips:

> The probability is $0.205\%$, as we saw.  However, if we **actually** flip the coin 10 times, there are only 2 outcomes:

- we get 4 heads (out of 10 coin flips)
- we don't get 4 heads (out of 10)

So, if wanted to *randomly* experience that **calculated** probability, we would have to conduct **many** such trials to see the expected behavior.

> Each 10-coin-flip is only **one** experiment.

In order to get a curve that we can play with like the above examples, we need to provide the following:

- The probability of success (e.g., the chance of getting a "heads")
- The number of times we repeat the event (e.g., the number of times we flip the coin)
- The number of times we repeat the ***trial***.  This is the trickier concept:
    - Like in the case above, each 10-coin-flip was only ***one*** trial (or event).
    - If we wanted to see the shape of the curve that represents the **expected** probabilities, we need to run many experiments.
    - This is just like the "normal" and "uniform" cases earlier.
        - The "normal" did not have its bell curve shape until around 1000 events (days).
        - Similarly, the "uniform" distribution needed many trials (die rolls).

Below is an interactive graphic showing all of these parameters to play around with:


```{r binom, echo=FALSE}
shinyAppFile("./Probability_2_binomial.R",
  options = list(
    width = "100%", height = 700
  )
)
```

# Continuation...

Click here to see the next stuff...



---
title: "Math Club - Probability, Part 2"
author: "Mike Williamson"
date: "May 18, 2015"
runtime: shiny
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
  word_document: default
fontsize: 12pt
---

```{r echo=FALSE, message=FALSE}
library(ggplot2)
```

This document is a continuation of the Math Club - Probability, Part 1.

#Looking at Different Distributions

## Uniform Distribution

We've been talking about "the roll of a die" or "flipping a coin".  We've also looked at calculating the ***combinations*** of probabilites, whether as a *union* or as succeeding (*"given that"*) events.  In all these scenarios, we studied the **uniform** distribution.  This is where the ***likelihood of each outcome is the same***.

It's the same chance ($\frac{1}{6}$) to roll a 1, or a 2, or any side of a die.  But what about things that are *random* but not *uniform*?  We'll look at some of them next

-----


##Playing with sample size

But first, let's look at "sample size":

When we played the "gambling game" (mentioned in the final section, **Games**), I specifically had you all roll the pair of dice 100 times.  This is because I wanted enough rolls so that things would turn out "as expected".

Let's make that interactive.  Below is a graph which gives the "measured probability" of each outcome, which is actually just the number of times that outcome occurred, divided by the number of times all outcomes occurred.

E.g., if you choose $10$ samples, and the measured probability of $2$ is $0.3$, then it means that $2$ showed up $3$ times ($10 \times 3 = 0.3$).


```{r echo=FALSE}
selectInput("n_samples", label = "Number of Samples:",
            choices = c(1:5, 10, 20, 50, 1e2, 1e3, 1e4, 1e5),
            selected = 10)
```

```{r echo=FALSE}
renderPlot({
  ggplot(data.frame(die_roll=sample(x = 1:6, size = as.numeric(input$n_samples),
                                    replace = TRUE)),
         aes(x = die_roll)) +
    geom_histogram(aes(y= ..density.., fill = ..count..), binwidth = 1,
                   color = "darkgreen", origin = -0.5) +
    labs(title = paste("# Times on a 6-Sided Die, out of", input$n_samples),
         y = "Measured Probability", x = "Die Roll") + xlim(0, 7) +
    theme(text = element_text(size = 23))
})
```

But as you can see by playing around with the number of samples below, sometimes even 100 samples, or tries, aren't enough to give the "real" shape of the *uniform distribution*, where we know each outcome is equally likely.

We **know** the probability is the same for each number to appear, and that the probability is $\frac{1}{6}$, or `r signif(1/6, 4)`, for the 6-sided die above.  But until the number of samples gets quite large, say around 1000 or so, we don't really see the lines flatten out and settle at $\frac{1}{6}$ very well.

-----


## Normal Distribution

At first you might have to think about it a bit, but then you'll realize there are a **ton** of cases where things are random, but not uniform:

* The height of 11 year olds
    * Very few are 3\` tall or 6\` tall, but many of you are around 4.5\` tall or so.

* The temperature mid-day in May by us is typically around 70$^{\circ}$.  It might be as cold as 55$^{\circ}$ every now and again, but it won't dip down to 45$^{\circ}$.

* The time it takes to finish your Mandarin homework.  One week it might take only 45 minutes for the whole week, the next week it might take a full 2 hours, but it won't ever take 100 hours.

* Etc.

These sorts of distributions, where things occur more often "near the middle, typical value" are called ["*Normal*"](http://en.wikipedia.org/wiki/Normal_distribution) or "*Gaussian*" distributions.

Below is a mock-up of one such *normal distribution*:

> the average high temperature for a particular day here in San Mateo.

It should maybe make sense that this would follow a normal distribution.  Normal distributions are those where certain values are more likely, and other values are less likely as they drift further from that central, more likely value.

If the "typical" temperature is, say, 70^o^, then 69^o^ or 68^o^ is not unexpected, but 50^o^ is.


```{r echo=FALSE}
selectInput("n_days", label = "Number of Samples:",
            choices = c(1:5, 10, 20, 50, 1e2, 1e3, 1e4, 1e5),
            selected = 10)
```

```{r echo=FALSE}
renderPlot({
  ggplot(data.frame(Temperature=rnorm(as.numeric(input$n_days), 70, 6)),
         aes(x = Temperature)) +
    geom_histogram(aes(y= ..density.., fill = ..count..),
                   color = "darkred") +
    labs(title = paste("Temperature High in mid-May,", input$n_days, "Measurements"),
         y = "Measured Probability", x = "Temperature") + 
    scale_fill_gradient(low="white", high="red") + 
    theme(text = element_text(size = 23))
})
```

*(For the curious, I found that the historical average high on May 15th is 70$^{\circ}$, and that the lowest recorded high was 50$^{\circ}$.  Assuming $\sim$ 3000 measurements - 150 years' data, where about 20 days in May are approximately the same - a standard deviation of around 6 would yield a "lowest high" of 50$^{\circ}$.  So I said the standard deviation is 6$^{\circ}$.)*

-----

## Binomial Distribution

As we've seen, the **normal** distribution has a symmetric, bell-curve shape.

The **uniform** distribution is, well, uniform.  That is, it is flat, with the same probability for each value.

But there are other random distributions that can occur.  Namely, the binomial distribution, which we'll look at now.  It looks something like:

```{r echo=FALSE}
renderPlot({
  ggplot(data.frame(Whatever=rbinom(1e5, 100, 0.025)), aes(x = Whatever)) +
    geom_histogram(aes(y= ..density.., fill = ..count..), color = "darkgreen") +
    guides(fill = FALSE) + 
    labs(title = "Binomial Distribution") +
    scale_fill_gradient(low="white", high="green") + 
    theme(text = element_text(size = 23))
})
```


### When would we use it?

The ***[binomial distribution](http://en.wikipedia.org/wiki/Binomial_distribution)*** allows us to go back to our gambling obsession.

Let's go back to our flipping of heads or tails on a coin.  I have a $50\%$ chance of getting a heads.  And I have a $25\%$ chance of getting two heads in a row.  Sure, we figured that out earlier.  Easy enough.

> But what's my chance of getting 2 heads out of 3 coin flips?

> Or what about getting 6 ***or more*** heads out of 10 coin flips?

Well, before we jump into the technical verbiage, let's think about some of the intuitive things we know.  If I set up a generic expression along the lines of:

> What's the chance of getting "X" successes out of "Y" attempts?

then what can I say about $X$, $Y$, and $P$?

- If $X > Y$, then $P = 0$.
    - Well, that's kinda obvious.  E.g., I can't get, say, 12 heads from only 10 coin flips, right?
- If $X = Y$ or $X = 0$, then there is **only** one way that can happen.
    - E.g., there is only one way I can get 8 heads from 8 coin flips... each flip must be a head.
- If $0 < X < Y$, then there will be **more than** one way that can happen.
    - E.g., if I need 4 heads from 7 coin flips, I could first get a tails, then a heads, then maybe 3 more tails in a row, and 2 more heads.  I could also get all 4 tails in a row, then all heads.  Etc.

-----

### A Simple Example

OK, so let's look again at the proposal:

> What's my chance of getting 2 heads out of 3 coin flips?

First, there are ***3 successive events*** happening, and each event has a ***0.5 probability***.

How did I get that?

- There are 3 coin flips
- Whether the coin reveals heads or tails, it is still a $50\%$ chance.
    - Later we'll talk about cases where one of the outcomes is more likely than the other.

So, using our knowledge so far, we have, for any particular case (say, 2 heads and then a tails):

$$P_{HHT} = P\left(heads\right) \times P\left(heads\right) \times P\left(tails\right)$$

$$P_{HHT} = \frac{1}{2} \times \frac{1}{2} \times \frac{1}{2} = \frac{1}{8}$$

But, we know that there are ***other ways*** to get 2 heads out of 3 coin flips.  For instance, we could get a tails first, and then 2 heads:

$$P_{THH} = P\left(tails\right) \times P\left(heads\right) \times P\left(heads\right)$$

We know from our **"Adding" Probabilities** section that we will need to *union* these possible scenarios.  Moreover, we know that if "heads-heads-tails" happens, then "tails-heads-heads" cannot happen, and vice versa.  In other words, these possibilities are *mutually exclusive*, so that when we *union* these, we can simply add them.

But there are yet more possibilities to get 2 heads.  We could also get "heads-tails-heads".  Those are all the possible ways to get 2 heads from 3 coin flips.

So:

$$P\left(2\ heads\ \mid\ 3\ flips\right) = P_{HHT} + P_{THH} + P_{HTH}$$

$$P\left(2\ heads\ \mid\ 3\ flips\right) = \frac{1}{8} + \frac{1}{8} + \frac{1}{8} = \frac{3}{8}$$


We got our answer:

> What's my chance of getting 2 heads out of 3 coin flips?  It's $\frac{3}{8}$!

-----

### Calculating Probability for Binomial Distribution

But what about

> the chance of getting, say, 4 heads out of 10 coin flips??

We *could* list out all the possibilities, but there has to be an easier way.  In fact, there is an easier way.  The **[binomial coefficient](http://en.wikipedia.org/wiki/Binomial_coefficient)**, or **[Pascal's triangle](http://en.wikipedia.org/wiki/Pascal%27s_triangle)**, provides an easy way to capture the number of possibilities:

$$\binom{n}{k} = \frac{n!}{k! \times \left(n - k\right)}$$

Plugging this into our 2 heads out of 3 coin flips, we get:

$$\binom{3}{2} = \frac{3!}{2! \times \left(3 - 2\right)!} = \frac{3 \times 2!}{2! \times 1!} = 3$$

Woot!  We get the right answer!  (Remember, we had 3 possible ways of getting 2 heads from three flips:  HHT, HTH, THH.)

Remember, we pronounce the binomial coefficient using the expression "choose".  E.g., "3 choose 2" is $\binom{3}{2}$.

OK, now with 4 heads out of 10 coin flips:

$$\binom{10}{4} = \frac{10!}{4! \times \left(10 - 4\right)!} = $$

$$\frac{10 \times 9 \times 8 \times 7 \times 6!}{4! \times 6!} = $$

$$\frac{10 \times 9 \times 8 \times 7}{4!} = $$

$$\frac{5040}{24} = 210$$

So, there are 210 different ways to get 4 heads from 10 coin flips.  Whew!  Good thing we didn't try to list them all!

Now, let's calculate the likelihood of any of those ways happening.  Since they are all the same, let's just choose 4 heads, then 6 tails:

$$P_{HHHHTTTTTT} = \frac{1}{2}^{10} = 0.000977$$

With 210 possibilities, since we know they are all *mutually exclusive*, we can add them all up.  And since they are all equally likely (i.e., the chance for $P_{HHHHTTTTTT}$ is the same as for $P_{TTHTTHHTTH}$), they will all have the same probability.

So...

> What is the chance of getting 4 heads out of 10 coin flips??

> $P\left(4\ heads \mid 10\ coin\ flips\right) = 210 \times 0.000977 = 0.205$

Or just over 20% chance of getting 4 heads out of 10 coin flips.

-----

Quick diversion, some fun with the **binomial coefficient**:  the binomial coefficient follows Pascal's Triangle.  For example, below is Pascal's Triangle going all the way from 0 - 10 different coin flips:

```{r echo=FALSE}
pascalTriangle <- function(h) {
  for(i in 0:(h-1)) {
    s <- as.character(i)
    for(k in 0:(h-i)) s <- paste(s, "  ", sep="")
    for(j in 0:i) {
      s <- paste(s, sprintf("%3d ", choose(i, j)), sep="")
    }
    print(s)
  }
}
pascalTriangle(11)
```

To generate it, you start with just the number 1 on the top row (for 0 trials), then the pair of numbers 1 & 1 on the second row (for 1 trial, which could either succeed or fail).

Every row after that is made by combining the values above it, and adding a 1 on each end.  For instance, the third row (2 trials) will have $1 + 1 = 2$ as it's middle value, and is bookended by 1's.  The fourth row (3 trials), will have $1 + 2 = 3$ in both of the cases where that happens, bookended by 1's.  Etc.

Each column on each row is the number of successes, starting with none and ending with all of them.  So, for the 11th row (10 trials), we have:

$$1\ 10\ 45\ 120\ 210\ 252\ 210\ 120\ 45\ 10\ 1$$

Where the $1$'s on the ends represent, respectively, no heads and all heads.  The $10$ represents 1 head (or 9 heads), $45$ is 2 heads, $120$ is 3 heads, and $210$ is 4 heads.

-----

### The Shape of the Binomial Distribution

The binomial distribution has a few more variables to futz with, in order to describe it, than we've seen in our other distributions.

Let's think through this via the previous example, the chance of getting 4 heads out of 10 coin flips:

> The probability is $0.205\%$, as we saw.  However, if we **actually** flip the coin 10 times, there are only 2 outcomes:

- we get 4 heads (out of 10 coin flips)
- we don't get 4 heads (out of 10)

So, if wanted to *randomly* experience that **calculated** probability, we would have to conduct **many** such trials to see the expected behavior.

> Each 10 coin flip is only **one** experiment.

In order to get a curve that we can play with like the above examples, we need to provide the following:

- The probability of success (e.g., the chance of getting a "heads")
- The number of times we repeat the event (e.g., the number of times we flip the coin)
- The number of times we repeat the ***trial***.  This is the trickier concept:
    - Like in the case above, each 10 coin flip was only ***one*** trial (or event).
    - If we wanted to see the shape of the curve that represents the **expected** probabilities, we need to run many experiments.
    - This is just like the "normal" and "uniform" cases earlier.
        - The "normal" did not have its bell curve shape until around 1000 or so events (days).
        - Similarly, the "uniform" distribution needed many trials (die rolls).

Below is an interactive graph showing all of these parameters to play around with:


```{r echo=FALSE}
sliderInput("success", label = "Probability of (event) Success:",
            min = 0, max = 1, value = 0.2, step = 0.02)

selectInput("n_events", label = "Number of Events (per trial):",
            choices = c(1:5, 10, 20, 30, 40, 50, 75, 100),
            selected = 20)

selectInput("n_trials", label = "Total Number of (repeated) Trials:",
            choices = c(1:5, 10, 20, 50, 1e2, 1e3, 1e4, 1e5),
            selected = 10)

renderPlot({
  ggplot(data.frame(Successes=rbinom(as.numeric(input$n_trials),
                                    as.numeric(input$n_events),
                                    as.numeric(input$success))),
         aes(x = Successes)) +
    geom_histogram(aes(y= ..density.., fill = ..count..),
                   color = "darkgreen") +
    labs(title = paste0("Number of Successes, given:\n",
                        input$n_trials, " trials, ",
                        input$n_events, " events per trial, and ",
                        input$success, "% chance per event"),
         y = "Measured Probability", x = "Number of Successes") + 
    scale_fill_gradient(low="white", high="green") + 
    theme(text = element_text(size = 21))
})
```

# Continuation...

Click here to see the next stuff...


